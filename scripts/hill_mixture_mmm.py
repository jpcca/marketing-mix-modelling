# -*- coding: utf-8 -*-
"""hill_mixture_mmm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AgcjKkvbFmy_9II4SiJruxCvjs4mX79j

# Hill Mixture Marketing Mix Model (MMM)

## Overview
This notebook implements a Bayesian Marketing Mix Model using a mixture of Hill saturation curves. The model is designed to:
- Measure the effectiveness of different marketing channels
- Account for diminishing returns using Hill saturation curves
- Incorporate carryover effects through adstock transformations
- Handle multiple marketing scenarios with mixture modeling
- Provide uncertainty quantification through Bayesian inference
"""

import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")

import arviz as az
import jax
import jax.numpy as jnp
import numpyro
import numpyro.distributions as dist
from jax import lax
from numpyro.infer import MCMC, NUTS, Predictive

# 複数chainを使うためのデバイス設定
NUM_CHAINS = 4
numpyro.set_host_device_count(NUM_CHAINS)

print(f"JAX devices: {jax.devices()}")
print(f"Using {NUM_CHAINS} chains for MCMC")

"""## 1. Core Functions"""


def adstock_geometric(x, alpha):
    """Geometric decay adstock transformation"""

    def step(carry, x_t):
        carry = x_t + alpha * carry
        return carry, carry

    _, s = lax.scan(step, 0.0, x)
    return s


def hill(x, A, k, n):
    """Hill saturation function"""
    return A * (x**n) / (k**n + x**n + 1e-12)


def hill_matrix(s, A, k, n):
    """
    Compute Hill values for all components

    Args:
        s: (T,) adstocked spend
        A: (K,) max effect
        k: (K,) half-saturation
        n: (K,) hill coefficient

    Returns:
        (T, K) hill effect matrix
    """
    s_col = s[:, None]  # (T, 1)
    return (
        A[None, :]
        * (s_col ** n[None, :])
        / (k[None, :] ** n[None, :] + s_col ** n[None, :] + 1e-12)
    )


"""## 2. Automatic Prior Configuration

実データに対応するため、`y` と `x` のスケールから適切なpriorを自動計算
"""


def compute_prior_config(x, y):
    """
    y と x のスケールから適切なpriorパラメータを自動計算

    Returns:
        dict with prior configuration
    """
    y_mean = np.mean(y)
    y_std = np.std(y)
    y_range = np.max(y) - np.min(y)

    x_median = np.median(x)
    x_max = np.max(x)

    config = {
        # Baseline priors (centered on y)
        "intercept_loc": float(y_mean),
        "intercept_scale": float(y_std * 2),  # 広めに
        "slope_scale": float(y_std),
        # A (max effect): y_rangeの一部を想定
        "A_loc": float(np.log(y_range * 0.3 + 1e-6)),  # y_rangeの30%程度
        "A_scale": 0.8,
        # k (half-saturation): x のスケールに合わせる
        "k_base_loc": float(np.log(x_median + 1e-6)),
        "k_scale": 0.7,
        # sigma: y_stdの一部
        "sigma_scale": float(y_std),
        # Reference values for diagnostics
        "x_median": float(x_median),
        "x_max": float(x_max),
        "y_mean": float(y_mean),
        "y_std": float(y_std),
    }

    return config


def print_prior_config(config):
    """Prior設定を表示"""
    print("Prior Configuration (auto-computed from data):")
    print(
        f"  intercept ~ Normal({config['intercept_loc']:.2f}, {config['intercept_scale']:.2f})"
    )
    print(f"  slope ~ Normal(0, {config['slope_scale']:.2f})")
    print(
        f"  A ~ LogNormal({config['A_loc']:.2f}, {config['A_scale']:.2f})  [median: {np.exp(config['A_loc']):.2f}]"
    )
    print(
        f"  k ~ LogNormal({config['k_base_loc']:.2f}, {config['k_scale']:.2f})  [median: {np.exp(config['k_base_loc']):.2f}]"
    )
    print(f"  sigma ~ HalfNormal({config['sigma_scale']:.2f})")
    print("  alpha ~ Beta(2, 2)  [mean: 0.5]")


"""## 1. Core Mathematical Functions

This section defines the fundamental mathematical transformations used in the MMM:

### Adstock Transformation
The adstock function models how marketing effects carry over time using geometric decay:
- Current period effect = current spend + α × previous period's effect
- α (alpha) controls the decay rate (0 = no carryover, 1 = full persistence)

### Hill Saturation Function
Models diminishing returns in marketing effectiveness:
- S(x) = x^α / (x^α + λ^α)
- λ (lambda) = half-saturation point (spend level at 50% effectiveness)
- α (alpha) = shape parameter (controls curve steepness)

"""


def generate_data_gmm_hill(seed=42, T=200):
    """
    Generate MMM data with GMM-style Hill mixture.

    DGP:
    - z_t ~ Categorical(pi)  # latent component
    - y_t ~ Normal(baseline_t + hill(s_t; A_{z_t}, k_{z_t}, n_{z_t}), sigma)
    """
    rng = np.random.default_rng(seed)

    # Spend (input)
    x = rng.lognormal(mean=1.5, sigma=0.6, size=T).astype(np.float32)

    # Adstock transformation
    alpha_true = 0.5
    s = np.array(adstock_geometric(jnp.array(x), jnp.array(alpha_true)))

    # Baseline (trend)
    t = np.arange(T, dtype=np.float32)
    t_std = (t - t.mean()) / (t.std() + 1e-6)
    intercept_true = 50.0
    slope_true = 2.0
    baseline = intercept_true + slope_true * t_std

    # True GMM-Hill parameters
    K_true = 3
    pi_true = np.array([0.40, 0.30, 0.30], dtype=np.float32)

    # k: half-saturation points (ordered: k_1 < k_2 < k_3)
    s_median = np.median(s)
    s_max = np.max(s)
    k_true = np.array(
        [s_median * 0.5, s_median * 1.0, s_median * 1.2], dtype=np.float32
    )

    # A: max effects
    A_true = np.array([15.0, 30.0, 60.0], dtype=np.float32)

    # n: hill coefficients
    n_true = np.array([2.0, 1.5, 1.0], dtype=np.float32)

    # Latent component assignment
    z_true = rng.choice(K_true, size=T, p=pi_true)

    # Compute effects based on latent assignment
    hill_mat = np.array(
        hill_matrix(
            jnp.array(s), jnp.array(A_true), jnp.array(k_true), jnp.array(n_true)
        )
    )
    effects = hill_mat[np.arange(T), z_true]

    # Generate observations
    mu_true = baseline + effects
    sigma_true = 3.0
    y = rng.normal(loc=mu_true, scale=sigma_true).astype(np.float32)

    meta = {
        "K_true": K_true,
        "pi_true": pi_true,
        "k_true": k_true,
        "A_true": A_true,
        "n_true": n_true,
        "sigma_true": sigma_true,
        "alpha_true": alpha_true,
        "intercept_true": intercept_true,
        "slope_true": slope_true,
        "s_median": s_median,
        "s_max": s_max,
        "z_true": z_true,
        "mu_true": mu_true,
        "hill_mat": hill_mat,
        "baseline": baseline,
        "s": s,
    }

    return x, y, s, meta


# Generate data
x, y, s_true, meta = generate_data_gmm_hill(seed=42, T=200)

print("=" * 60)
print("Data Generation Summary")
print("=" * 60)
print(f"T = {len(y)} observations")
print(f"K_true = {meta['K_true']} components")
print("\nTrue parameters:")
print(f"  alpha = {meta['alpha_true']}")
print(f"  pi    = {meta['pi_true']}")
print(f"  k     = {meta['k_true'].round(2)}")
print(f"  A     = {meta['A_true']}")
print(f"  n     = {meta['n_true']}")
print(f"  sigma = {meta['sigma_true']}")
print("\nAdstock spend stats:")
print(f"  median(s) = {meta['s_median']:.2f}")
print(f"  max(s)    = {meta['s_max']:.2f}")

# Visualize data
fig, axes = plt.subplots(1, 2, figsize=(15, 4))

colors = ["#1f77b4", "#ff7f0e", "#2ca02c"]

# Plot 1: True response curves
ax = axes[0]
s_range = np.linspace(0, meta["s_max"] * 1.2, 100)
for k in range(meta["K_true"]):
    resp = hill(s_range, meta["A_true"][k], meta["k_true"][k], meta["n_true"][k])
    ax.plot(
        s_range,
        resp,
        color=colors[k],
        linewidth=2.5,
        label=f"K={k}: k={meta['k_true'][k]:.1f}, A={meta['A_true'][k]:.0f}",
    )
ax.axvline(meta["s_median"], color="gray", linestyle="--", alpha=0.5, label="median(s)")
ax.set_xlabel("Adstocked Spend")
ax.set_ylabel("Effect")
ax.set_title("True Hill Response Curves")
ax.legend(fontsize=8)
ax.grid(True, alpha=0.5)

# Plot 2: Observations by component
ax = axes[1]
for k in range(meta["K_true"]):
    mask = meta["z_true"] == k
    ax.scatter(
        np.where(mask)[0],
        y[mask],
        c=colors[k],
        label=f"K={k} (n={mask.sum()})",
        alpha=0.6,
        s=20,
    )
ax.set_xlabel("Time")
ax.set_ylabel("Sales")
ax.set_title("Observations by True Component")
ax.legend(fontsize=8)
ax.grid(True, alpha=0.5)


plt.tight_layout()
plt.show()


def model_single_hill_v3(x, y=None, prior_config=None):
    """
    Single Hill function MMM with alpha inference

    x -> s (via alpha) -> Hill -> y
    """
    T = x.shape[0]
    t = jnp.arange(T, dtype=jnp.float32)
    t_std = (t - jnp.mean(t)) / (jnp.std(t) + 1e-6)

    # Default prior config
    if prior_config is None:
        prior_config = {
            "intercept_loc": 50.0,
            "intercept_scale": 20.0,
            "slope_scale": 5.0,
            "A_loc": np.log(30.0),
            "A_scale": 0.8,
            "k_base_loc": np.log(10.0),
            "k_scale": 0.7,
            "sigma_scale": 10.0,
        }

    # ===== Adstock parameter =====
    alpha = numpyro.sample("alpha", dist.Beta(2, 2))

    # Compute adstocked spend
    s = adstock_geometric(x, alpha)
    numpyro.deterministic("s", s)

    # ===== Baseline =====
    intercept = numpyro.sample(
        "intercept",
        dist.Normal(prior_config["intercept_loc"], prior_config["intercept_scale"]),
    )
    slope = numpyro.sample("slope", dist.Normal(0.0, prior_config["slope_scale"]))
    baseline = intercept + slope * t_std

    # ===== Hill parameters =====
    s_median = jnp.median(s)

    A = numpyro.sample(
        "A", dist.LogNormal(prior_config["A_loc"], prior_config["A_scale"])
    )
    k = numpyro.sample(
        "k", dist.LogNormal(jnp.log(s_median + 1e-6), prior_config["k_scale"])
    )
    n = numpyro.sample("n", dist.LogNormal(jnp.log(1.5), 0.4))
    sigma = numpyro.sample("sigma", dist.HalfNormal(prior_config["sigma_scale"]))

    # Effect
    effect = hill(s, A, k, n)
    mu = baseline + effect

    with numpyro.plate("time", T):
        numpyro.sample("y", dist.Normal(mu, sigma), obs=y)

    numpyro.deterministic("mu", mu)
    numpyro.deterministic("effect", effect)


"""## 4. Models with Alpha Inference"""


def model_hill_mixture_v3(x, y=None, K=3, prior_config=None):
    """
    Hill Mixture Model v3 - GMM-based with:
    1. Alpha inference (x -> s inside model)
    2. Auto-scaled priors based on data
    3. Ordered constraint on k (half-saturation) via cumsum
    """
    T = x.shape[0]
    t = jnp.arange(T, dtype=jnp.float32)
    t_std = (t - jnp.mean(t)) / (jnp.std(t) + 1e-6)

    # Default prior config
    if prior_config is None:
        prior_config = {
            "intercept_loc": 50.0,
            "intercept_scale": 20.0,
            "slope_scale": 5.0,
            "A_loc": np.log(30.0),
            "A_scale": 0.8,
            "k_base_loc": np.log(10.0),
            "k_scale": 0.7,
            "sigma_scale": 10.0,
            "x_max": 50.0,
        }

    # ===== Adstock parameter =====
    alpha = numpyro.sample("alpha", dist.Beta(2, 2))

    # Compute adstocked spend
    s = adstock_geometric(x, alpha)
    numpyro.deterministic("s", s)

    # ===== Baseline =====
    intercept = numpyro.sample(
        "intercept",
        dist.Normal(prior_config["intercept_loc"], prior_config["intercept_scale"]),
    )
    slope = numpyro.sample("slope", dist.Normal(0.0, prior_config["slope_scale"]))
    baseline = intercept + slope * t_std

    # ===== Mixture weights =====
    pis = numpyro.sample("pis", dist.Dirichlet(jnp.ones(K)))

    # ===== Hill parameters with identifiability constraints =====
    s_max = jnp.max(s)

    # k: half-saturation (ordered via cumsum)
    k_delta = numpyro.sample(
        "k_delta",
        dist.LogNormal(jnp.log(s_max / (K + 1) + 1e-6), prior_config["k_scale"]).expand(
            [K]
        ),
    )
    k = jnp.cumsum(k_delta)
    numpyro.deterministic("k", k)

    # A: max effect (independent per component)
    A = numpyro.sample(
        "A", dist.LogNormal(prior_config["A_loc"], prior_config["A_scale"]).expand([K])
    )

    # n: hill coefficient
    n = numpyro.sample("n", dist.LogNormal(jnp.log(1.5), 0.4).expand([K]))

    # sigma: observation noise (shared)
    sigma = numpyro.sample("sigma", dist.HalfNormal(prior_config["sigma_scale"]))

    # ===== Hill transformation =====
    hill_mat = hill_matrix(s, A, k, n)  # (T, K)
    mu_components = baseline[:, None] + hill_mat  # (T, K)

    # ===== Likelihood (GMM-style) =====
    with numpyro.plate("time", T):
        numpyro.sample(
            "y",
            dist.MixtureSameFamily(
                dist.Categorical(pis), dist.Normal(mu_components, sigma)
            ),
            obs=y,
        )

    # ===== Deterministic quantities for analysis =====
    mu_expected = baseline + jnp.sum(pis * hill_mat, axis=1)
    numpyro.deterministic("mu_expected", mu_expected)
    numpyro.deterministic("hill_components", hill_mat)
    numpyro.deterministic("mu_components", mu_components)


def model_hill_mixture_sparse(x, y=None, K=5, prior_config=None):
    """
    Hill Mixture Model with sparse Dirichlet prior.
    Allows automatic pruning of unnecessary components.
    """
    T = x.shape[0]
    t = jnp.arange(T, dtype=jnp.float32)
    t_std = (t - jnp.mean(t)) / (jnp.std(t) + 1e-6)

    # Default prior config
    if prior_config is None:
        prior_config = {
            "intercept_loc": 50.0,
            "intercept_scale": 20.0,
            "slope_scale": 5.0,
            "A_loc": np.log(30.0),
            "A_scale": 0.8,
            "k_base_loc": np.log(10.0),
            "k_scale": 0.7,
            "sigma_scale": 10.0,
        }

    # ===== Adstock parameter =====
    alpha = numpyro.sample("alpha", dist.Beta(2, 2))
    s = adstock_geometric(x, alpha)
    numpyro.deterministic("s", s)

    # Baseline
    intercept = numpyro.sample(
        "intercept",
        dist.Normal(prior_config["intercept_loc"], prior_config["intercept_scale"]),
    )
    slope = numpyro.sample("slope", dist.Normal(0.0, prior_config["slope_scale"]))
    baseline = intercept + slope * t_std

    # Sparse Dirichlet (concentration < 1 encourages sparsity)
    pis = numpyro.sample("pis", dist.Dirichlet(jnp.ones(K) * 0.5))

    # Hill parameters
    s_max = jnp.max(s)

    k_delta = numpyro.sample(
        "k_delta",
        dist.LogNormal(jnp.log(s_max / (K + 1) + 1e-6), prior_config["k_scale"]).expand(
            [K]
        ),
    )
    k = jnp.cumsum(k_delta)
    numpyro.deterministic("k", k)

    A = numpyro.sample(
        "A", dist.LogNormal(prior_config["A_loc"], prior_config["A_scale"]).expand([K])
    )
    n = numpyro.sample("n", dist.LogNormal(jnp.log(1.5), 0.4).expand([K]))
    sigma = numpyro.sample("sigma", dist.HalfNormal(prior_config["sigma_scale"]))

    # Hill transformation
    hill_mat = hill_matrix(s, A, k, n)
    mu_components = baseline[:, None] + hill_mat

    with numpyro.plate("time", T):
        numpyro.sample(
            "y",
            dist.MixtureSameFamily(
                dist.Categorical(pis), dist.Normal(mu_components, sigma)
            ),
            obs=y,
        )

    mu_expected = baseline + jnp.sum(pis * hill_mat, axis=1)
    numpyro.deterministic("mu_expected", mu_expected)
    numpyro.deterministic("hill_components", hill_mat)


"""## 5. Inference Utilities with Multi-Chain Support"""


def run_inference(
    model_fn,
    x,
    y,
    seed=0,
    num_warmup=1000,
    num_samples=1000,
    num_chains=4,
    prior_config=None,
    **kwargs,
):
    """
    Run MCMC inference with multiple chains

    Note: Model now takes x directly (not s), since adstock is computed inside
    """
    rng_key = jax.random.PRNGKey(seed)
    kernel = NUTS(model_fn, target_accept_prob=0.9)
    mcmc = MCMC(
        kernel,
        num_warmup=num_warmup,
        num_samples=num_samples,
        num_chains=num_chains,
        progress_bar=True,
    )
    mcmc.run(
        rng_key, x=jnp.array(x), y=jnp.array(y), prior_config=prior_config, **kwargs
    )
    return mcmc


def compute_predictions(mcmc, model_fn, x, prior_config=None, **kwargs):
    """Compute posterior predictive"""
    samples = mcmc.get_samples()
    pred = Predictive(model_fn, posterior_samples=samples)
    pred_samples = pred(
        jax.random.PRNGKey(99),
        x=jnp.array(x),
        y=None,
        prior_config=prior_config,
        **kwargs,
    )
    return pred_samples


def compute_metrics(y_true, y_pred_samples):
    """Compute RMSE and 90% prediction intervals"""
    y_samples = np.array(y_pred_samples)
    y_pred_mean = y_samples.mean(axis=0)

    # 90% prediction interval
    q05 = np.quantile(y_samples, 0.05, axis=0)
    q95 = np.quantile(y_samples, 0.95, axis=0)

    # Coverage
    coverage = np.mean((y_true >= q05) & (y_true <= q95))

    # RMSE
    rmse = np.sqrt(np.mean((y_pred_mean - y_true) ** 2))

    return {
        "rmse": rmse,
        "y_pred_mean": y_pred_mean,
        "q05": q05,
        "q95": q95,
        "coverage": coverage,
    }


"""## 6. Convergence Diagnostics (R-hat / ESS)"""


def print_convergence_diagnostics(mcmc, params=None):
    """
    Print R-hat and ESS diagnostics for key parameters

    Args:
        mcmc: MCMC object
        params: list of parameter names to check (default: auto-detect)
    """
    idata = az.from_numpyro(mcmc)

    # Get summary with R-hat and ESS
    summary = az.summary(idata, var_names=params, kind="diagnostics")

    print("\n" + "=" * 70)
    print("Convergence Diagnostics (R-hat and ESS)")
    print("=" * 70)
    print("\nGuidelines:")
    print("  - R-hat < 1.01: Good convergence")
    print("  - R-hat < 1.05: Acceptable")
    print("  - R-hat > 1.1: Poor convergence - consider more samples")
    print("  - ESS_bulk, ESS_tail > 400: Adequate effective samples")
    print("\n")

    # Display summary
    display(summary)

    # Check for convergence issues
    if "r_hat" in summary.columns:
        max_rhat = summary["r_hat"].max()
        bad_rhat = summary[summary["r_hat"] > 1.05]

        print(f"\nMax R-hat: {max_rhat:.4f}")
        if len(bad_rhat) > 0:
            print("\n⚠️  Parameters with R-hat > 1.05:")
            display(bad_rhat)
        else:
            print("✓ All parameters have R-hat < 1.05")

    if "ess_bulk" in summary.columns:
        min_ess = summary["ess_bulk"].min()
        print(f"Min ESS (bulk): {min_ess:.0f}")
        if min_ess < 400:
            print("⚠️  Some parameters have ESS < 400 - consider more samples")

    return summary


def plot_trace(mcmc, params=None):
    """Plot trace plots for diagnostics"""
    idata = az.from_numpyro(mcmc)
    az.plot_trace(
        idata,
        var_names=params,
        compact=True,
        figsize=(12, 2 * len(params) if params else 12),
    )
    plt.tight_layout()
    plt.show()


"""## 7. Comprehensive Model Evaluation"""


def evaluate_model_comprehensive(
    mcmc,
    model_fn,
    x,
    y,
    model_name="Model",
    prior_config=None,
    show_trace=False,
    **kwargs,
):
    """
    Comprehensive model evaluation including:
    - R-hat and ESS (convergence diagnostics)
    - RMSE and Coverage
    - LOO-CV (PSIS-LOO)
    - WAIC
    - Pareto k diagnostics
    """
    print(f"\n{'=' * 70}")
    print(f"Evaluation: {model_name}")
    print(f"{'=' * 70}")

    # Convert to ArviZ InferenceData
    idata = az.from_numpyro(mcmc)

    # 0. Convergence diagnostics
    # モデルに存在するパラメータのみを選択
    samples = mcmc.get_samples()
    key_params = ["alpha", "intercept", "slope", "sigma"]

    # Single Hill vs Mixture の判定
    if "k_delta" in samples:
        # Mixture model (k_delta を使用)
        key_params.extend(["A", "k_delta", "n"])
        if "pis" in samples:
            key_params.append("pis")
    elif "A" in samples:
        # Single Hill model (k を直接サンプリング)
        key_params.extend(["A", "k", "n"])

    diag_summary = print_convergence_diagnostics(mcmc, params=key_params)

    if show_trace:
        plot_trace(mcmc, params=key_params[:6])  # Limit to first 6 for visibility

    # 1. LOO-CV
    try:
        loo = az.loo(idata, pointwise=True)
        print("\n[LOO-CV]")
        print(f"  ELPD LOO: {loo.elpd_loo:.2f} (SE: {loo.se:.2f})")
        print(f"  p_loo (effective params): {loo.p_loo:.2f}")

        # Pareto k diagnostics
        pareto_k = loo.pareto_k.values
        bad_k = np.sum(pareto_k > 0.7)
        very_bad_k = np.sum(pareto_k > 1.0)
        print(
            f"  Pareto k > 0.7: {bad_k}/{len(pareto_k)} ({100 * bad_k / len(pareto_k):.1f}%)"
        )
        print(
            f"  Pareto k > 1.0: {very_bad_k}/{len(pareto_k)} ({100 * very_bad_k / len(pareto_k):.1f}%)"
        )
    except Exception as e:
        print(f"  LOO computation failed: {e}")
        loo = None

    # 2. WAIC
    try:
        waic = az.waic(idata)
        print("\n[WAIC]")
        print(f"  WAIC: {waic.elpd_waic:.2f} (SE: {waic.se:.2f})")
        print(f"  p_waic: {waic.p_waic:.2f}")
    except Exception as e:
        print(f"  WAIC computation failed: {e}")
        waic = None

    # 3. Predictive metrics
    pred = compute_predictions(mcmc, model_fn, x, prior_config=prior_config, **kwargs)
    metrics = compute_metrics(y, pred["y"])

    print("\n[Predictive Metrics]")
    print(f"  RMSE: {metrics['rmse']:.3f}")
    print(f"  90% PI Coverage: {metrics['coverage']:.1%}")

    # 4. Alpha estimate (key new parameter)
    samples = mcmc.get_samples()
    if "alpha" in samples:
        alpha_samples = np.array(samples["alpha"])
        print("\n[Alpha (Adstock) Estimate]")
        print(f"  Mean: {alpha_samples.mean():.3f}")
        print(f"  Std:  {alpha_samples.std():.3f}")
        print(
            f"  95% CI: [{np.percentile(alpha_samples, 2.5):.3f}, {np.percentile(alpha_samples, 97.5):.3f}]"
        )

    return {
        "model_name": model_name,
        "loo": loo,
        "waic": waic,
        "metrics": metrics,
        "idata": idata,
        "diagnostics": diag_summary,
    }


"""## 8. Run Models"""

# Train/Test split
T_train = 150
x_train, y_train = x[:T_train], y[:T_train]
x_test, y_test = x[T_train:], y[T_train:]

print(f"Train: {T_train} samples")
print(f"Test: {len(x_test)} samples")

# Compute prior configuration from training data
prior_config = compute_prior_config(x_train, y_train)
print("\n")
print_prior_config(prior_config)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# print("Training Single Hill (with alpha inference)...")
# mcmc_single = run_inference(
#     model_single_hill_v3, x_train, y_train,
#     seed=0, num_warmup=1000, num_samples=3000, num_chains=NUM_CHAINS,
#     prior_config=prior_config
# )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# print("Training Hill Mixture v3 (K=3, with alpha inference)...")
# mcmc_mix3 = run_inference(
#     model_hill_mixture_v3, x_train, y_train,
#     seed=0, num_warmup=1000, num_samples=3000, num_chains=NUM_CHAINS,
#     prior_config=prior_config, K=3
# )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# print("Training Sparse Hill Mixture v3 (K=5, with alpha inference)...")
# mcmc_sparse5 = run_inference(
#     model_hill_mixture_sparse, x_train, y_train,
#     seed=0, num_warmup=1000, num_samples=3000, num_chains=NUM_CHAINS,
#     prior_config=prior_config, K=5
# )

"""## 9. Model Evaluation"""

# Evaluate Single Hill
eval_single = evaluate_model_comprehensive(
    mcmc_single,
    model_single_hill_v3,
    x_train,
    y_train,
    model_name="Single Hill",
    prior_config=prior_config,
    show_trace=True,
)

# Evaluate Mixture K=3
eval_mix3 = evaluate_model_comprehensive(
    mcmc_mix3,
    model_hill_mixture_v3,
    x_train,
    y_train,
    model_name="Mixture K=3",
    prior_config=prior_config,
    K=3,
    show_trace=True,
)

# Evaluate Sparse K=5
eval_sparse5 = evaluate_model_comprehensive(
    mcmc_sparse5,
    model_hill_mixture_sparse,
    x_train,
    y_train,
    model_name="Sparse K=5",
    prior_config=prior_config,
    K=5,
    show_trace=True,
)

"""## 2. Bayesian Model Definition

### Model Architecture
This section defines the probabilistic model using NumPyro with the following components:

**Mixture Structure**:
- K=2 latent states representing different marketing regimes
- Dirichlet(1,1) prior for mixture weights
- Each component has independent Hill parameters

**Parameter Priors**:
- Intercept: Normal(0, 5)
- Beta coefficients: Normal(0, 1) for marketing effectiveness
- Hill λ: LogNormal based on spend range
- Hill α: Gamma(2, 1) for shape parameters
- Adstock α: Beta(2, 2) for decay rates
- Sigma: HalfNormal(1) for observation noise

**Likelihood**:
- Gaussian observation model
- Mixture components weighted by pi

"""


def check_parameter_recovery(mcmc, meta):
    """Compare estimated parameters with true values"""
    samples = mcmc.get_samples()

    print("\n" + "=" * 60)
    print("Parameter Recovery Check")
    print("=" * 60)

    # Alpha
    if "alpha" in samples:
        alpha_est = np.array(samples["alpha"])
        alpha_true = meta["alpha_true"]
        in_ci = (
            np.percentile(alpha_est, 2.5)
            <= alpha_true
            <= np.percentile(alpha_est, 97.5)
        )
        print("\nalpha:")
        print(f"  True: {alpha_true:.3f}")
        print(f"  Est:  {alpha_est.mean():.3f} ± {alpha_est.std():.3f}")
        print(
            f"  95% CI: [{np.percentile(alpha_est, 2.5):.3f}, {np.percentile(alpha_est, 97.5):.3f}]"
        )
        print(f"  True in CI: {'✓' if in_ci else '✗'}")

    # Sigma
    if "sigma" in samples:
        sigma_est = np.array(samples["sigma"])
        sigma_true = meta["sigma_true"]
        in_ci = (
            np.percentile(sigma_est, 2.5)
            <= sigma_true
            <= np.percentile(sigma_est, 97.5)
        )
        print("\nsigma:")
        print(f"  True: {sigma_true:.3f}")
        print(f"  Est:  {sigma_est.mean():.3f} ± {sigma_est.std():.3f}")
        print(
            f"  95% CI: [{np.percentile(sigma_est, 2.5):.3f}, {np.percentile(sigma_est, 97.5):.3f}]"
        )
        print(f"  True in CI: {'✓' if in_ci else '✗'}")

    # Intercept
    if "intercept" in samples:
        int_est = np.array(samples["intercept"])
        int_true = meta["intercept_true"]
        in_ci = np.percentile(int_est, 2.5) <= int_true <= np.percentile(int_est, 97.5)
        print("\nintercept:")
        print(f"  True: {int_true:.3f}")
        print(f"  Est:  {int_est.mean():.3f} ± {int_est.std():.3f}")
        print(
            f"  95% CI: [{np.percentile(int_est, 2.5):.3f}, {np.percentile(int_est, 97.5):.3f}]"
        )
        print(f"  True in CI: {'✓' if in_ci else '✗'}")


# Check for mixture model
check_parameter_recovery(mcmc_mix3, meta)

"""## 11. Model Comparison"""

# LOO comparison
print("\n" + "=" * 60)
print("LOO-CV Model Comparison")
print("=" * 60)

compare_dict = {}
for name, eval_result in [
    ("Single Hill", eval_single),
    ("Mixture K=3", eval_mix3),
    ("Sparse K=5", eval_sparse5),
]:
    if eval_result["loo"] is not None:
        compare_dict[name] = eval_result["idata"]

if len(compare_dict) > 1:
    comparison = az.compare(compare_dict, ic="loo")
    display(comparison)

    # Plot comparison
    az.plot_compare(comparison)
    plt.title("LOO-CV Model Comparison")
    plt.tight_layout()
    plt.show()

"""## 12. Test Set Evaluation"""


def evaluate_on_test(
    mcmc, model_fn, x_test, y_test, model_name, prior_config=None, **kwargs
):
    """Evaluate model on test set"""
    pred = compute_predictions(
        mcmc, model_fn, x_test, prior_config=prior_config, **kwargs
    )
    metrics = compute_metrics(y_test, pred["y"])

    return {
        "Model": model_name,
        "Test_RMSE": np.float32(metrics["rmse"]),
        "Test_Coverage": f"{metrics['coverage']:.1%}",
    }


test_results = [
    evaluate_on_test(
        mcmc_single,
        model_single_hill_v3,
        x_test,
        y_test,
        "Single Hill",
        prior_config=prior_config,
    ),
    evaluate_on_test(
        mcmc_mix3,
        model_hill_mixture_v3,
        x_test,
        y_test,
        "Mixture K=3",
        prior_config=prior_config,
        K=3,
    ),
    evaluate_on_test(
        mcmc_sparse5,
        model_hill_mixture_sparse,
        x_test,
        y_test,
        "Sparse K=5",
        prior_config=prior_config,
        K=5,
    ),
]

df_test = pd.DataFrame(test_results)
print("\n[Test Set Results]")
display(df_test)

# =============================================================================
# セル1: 時系列予測プロット（Train/Test連続表示版）
# =============================================================================


def plot_predictions_timeseries(
    mcmc_list,
    model_fns,
    names,
    colors,
    x_full,
    y_full,
    T_train,
    prior_config,
    kwargs_list,
):
    """
    時系列での予測プロット (Train + Test を連続表示)
    """
    T_full = len(y_full)
    time_full = np.arange(T_full)

    x_train, x_test = x_full[:T_train], x_full[T_train:]
    y_train, y_test = y_full[:T_train], y_full[T_train:]

    fig, axes = plt.subplots(
        len(mcmc_list), 1, figsize=(14, 4 * len(mcmc_list)), sharex=True
    )
    if len(mcmc_list) == 1:
        axes = [axes]

    for idx, (mcmc, model_fn, name, color, kwargs) in enumerate(
        zip(mcmc_list, model_fns, names, colors, kwargs_list)
    ):
        ax = axes[idx]

        # Full predictions (train + test を連続で予測)
        pred_full = compute_predictions(
            mcmc, model_fn, x_full, prior_config=prior_config, **kwargs
        )
        y_samples = np.array(pred_full["y"])
        y_pred_mean = y_samples.mean(axis=0)
        q05 = np.quantile(y_samples, 0.05, axis=0)
        q95 = np.quantile(y_samples, 0.95, axis=0)

        # Train/Test metrics
        coverage_train = np.mean(
            (y_train >= q05[:T_train]) & (y_train <= q95[:T_train])
        )
        coverage_test = np.mean((y_test >= q05[T_train:]) & (y_test <= q95[T_train:]))
        rmse_train = np.sqrt(np.mean((y_pred_mean[:T_train] - y_train) ** 2))
        rmse_test = np.sqrt(np.mean((y_pred_mean[T_train:] - y_test) ** 2))

        # 90% PI (連続)
        ax.fill_between(
            time_full[:T_train],
            q05[:T_train],
            q95[:T_train],
            color=color,
            alpha=0.2,
            label="Train 90% PI",
        )
        ax.fill_between(
            time_full[T_train:],
            q05[T_train:],
            q95[T_train:],
            color="red",
            alpha=0.2,
            label="Test 90% PI",
        )

        # Prediction line (連続)
        ax.plot(
            time_full, y_pred_mean, "-", color=color, linewidth=1.5, label="Prediction"
        )

        # Observations
        ax.plot(
            time_full[:T_train],
            y_train,
            "ko",
            markersize=3,
            alpha=0.5,
            label="Train obs",
        )
        ax.plot(
            time_full[T_train:], y_test, "rs", markersize=4, alpha=0.6, label="Test obs"
        )

        # Vertical line at train/test split
        ax.axvline(
            T_train,
            color="gray",
            linestyle="--",
            linewidth=2,
            alpha=0.7,
            label="Train/Test split",
        )

        ax.set_ylabel("Sales")
        ax.set_title(
            f"{name}: Train RMSE={rmse_train:.3f} (Cov={coverage_train:.1%}), "
            f"Test RMSE={rmse_test:.3f} (Cov={coverage_test:.1%})"
        )
        ax.legend(loc="upper left", fontsize=8)
        ax.grid(True, alpha=0.3)

    axes[-1].set_xlabel("Time")
    plt.tight_layout()
    plt.show()


# Plot predictions for all models
print("\n" + "=" * 70)
print("Time Series Predictions (Train + Test)")
print("=" * 70)

# x, y は全データを使用
plot_predictions_timeseries(
    mcmc_list=[mcmc_single, mcmc_mix3, mcmc_sparse5],
    model_fns=[model_single_hill_v3, model_hill_mixture_v3, model_hill_mixture_sparse],
    names=["Single Hill", "Mixture K=3", "Sparse K=5"],
    colors=["blue", "green", "orange"],
    x_full=x,
    y_full=y,
    T_train=T_train,
    prior_config=prior_config,
    kwargs_list=[{}, {"K": 3}, {"K": 5}],
)

"""## 13. Visualize Fitted Response Curves"""

# =============================================================================
# セル2: Hill関数プロット（全K成分表示版）
# =============================================================================


def plot_fitted_response(mcmc, meta, model_name="Model"):
    """Plot fitted response curves vs true curves (全成分表示)"""
    samples = mcmc.get_samples()

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Get estimated s from model
    s_est = np.array(samples["s"]).mean(axis=0)
    s_range = np.linspace(0, max(meta["s_max"], s_est.max()) * 1.2, 100)

    # Plot 1: Alpha estimate
    ax = axes[0]
    alpha_samples = np.array(samples["alpha"])
    ax.hist(alpha_samples, bins=50, density=True, alpha=0.7, color="steelblue")
    ax.axvline(
        meta["alpha_true"],
        color="red",
        linestyle="--",
        linewidth=2,
        label=f"True α={meta['alpha_true']}",
    )
    ax.axvline(
        alpha_samples.mean(),
        color="blue",
        linestyle="-",
        linewidth=2,
        label=f"Est α={alpha_samples.mean():.3f}",
    )
    ax.set_xlabel("Alpha")
    ax.set_ylabel("Density")
    ax.set_title(f"{model_name}: Alpha Posterior")
    ax.legend()

    # Plot 2: Response curves
    ax = axes[1]

    # カラーマップを使用して任意の数の成分に対応
    cmap_true = plt.cm.Set1
    cmap_est = plt.cm.Pastel1

    # True curves
    for k in range(meta["K_true"]):
        resp = hill(s_range, meta["A_true"][k], meta["k_true"][k], meta["n_true"][k])
        ax.plot(
            s_range,
            resp,
            color=cmap_true(k),
            linewidth=2,
            label=f"True K={k}",
            linestyle="--",
        )

    # Estimated curves
    if "A" in samples and len(np.array(samples["A"]).shape) > 1:
        # Mixture model
        A_mean = np.array(samples["A"]).mean(axis=0)
        k_mean = np.array(samples["k"]).mean(axis=0)
        n_mean = np.array(samples["n"]).mean(axis=0)
        K_est = len(A_mean)

        # pis があれば重みも表示
        if "pis" in samples:
            pis_mean = np.array(samples["pis"]).mean(axis=0)
        else:
            pis_mean = None

        # 全ての成分をプロット
        for k in range(K_est):
            resp = hill(s_range, A_mean[k], k_mean[k], n_mean[k])
            label = f"Est K={k}"
            if pis_mean is not None:
                label += f" (π={pis_mean[k]:.2f})"
            ax.plot(s_range, resp, color=cmap_est(k), linewidth=2.5, label=label)

    elif "A" in samples:
        # Single Hill model
        A_mean = np.array(samples["A"]).mean()
        k_mean = np.array(samples["k"]).mean()
        n_mean = np.array(samples["n"]).mean()
        resp = hill(s_range, A_mean, k_mean, n_mean)
        ax.plot(s_range, resp, color="black", linewidth=2.5, label="Estimated")

    ax.axvline(
        meta["s_median"], color="gray", linestyle=":", alpha=0.5, label="median(s)"
    )
    ax.set_xlabel("Adstocked Spend (s)")
    ax.set_ylabel("Effect")
    ax.set_title(f"{model_name}: Response Curves")
    ax.legend(fontsize=7, loc="upper left")
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


# Plot for each model
plot_fitted_response(mcmc_single, meta, "Single Hill")
plot_fitted_response(mcmc_mix3, meta, "Mixture K=3")
plot_fitted_response(mcmc_sparse5, meta, "Sparse K=5")

"""## 14. Final Summary"""

print("\n" + "=" * 70)
print("Final Summary")
print("=" * 70)

print("\n[Train Metrics]")
train_summary = []
for eval_result in [eval_single, eval_mix3, eval_sparse5]:
    row = {
        "Model": eval_result["model_name"],
        "RMSE": eval_result["metrics"]["rmse"],
        "Coverage": f"{eval_result['metrics']['coverage']:.1%}",
    }
    if eval_result["loo"] is not None:
        row["ELPD_LOO"] = eval_result["loo"].elpd_loo
        row["p_loo"] = eval_result["loo"].p_loo
    else:
        row["ELPD_LOO"] = None
        row["p_loo"] = None
    train_summary.append(row)

df_train = pd.DataFrame(train_summary)
display(df_train.round(3))

print("\n[Test Metrics]")
display(df_test)

print("\n[Alpha Recovery]")
alpha_summary = []
for name, mcmc in [
    ("Single Hill", mcmc_single),
    ("Mixture K=3", mcmc_mix3),
    ("Sparse K=5", mcmc_sparse5),
]:
    alpha_samples = np.array(mcmc.get_samples()["alpha"])
    alpha_summary.append(
        {
            "Model": name,
            "True_alpha": meta["alpha_true"],
            "Est_alpha": f"{alpha_samples.mean():.3f} ± {alpha_samples.std():.3f}",
            "95% CI": f"[{np.percentile(alpha_samples, 2.5):.3f}, {np.percentile(alpha_samples, 97.5):.3f}]",
        }
    )
display(pd.DataFrame(alpha_summary))
